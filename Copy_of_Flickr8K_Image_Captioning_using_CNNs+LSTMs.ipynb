{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chenjery007/ImageCaptioing/blob/main/Copy_of_Flickr8K_Image_Captioning_using_CNNs%2BLSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "hsankesara_flickr_image_dataset_path = kagglehub.dataset_download('hsankesara/flickr-image-dataset')\n",
        "adityajn105_flickr8k_path = kagglehub.dataset_download('adityajn105/flickr8k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "GEcTmaq83GoA",
        "outputId": "1bae81c4-d238-4cb1-abcb-46a50e640c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/hsankesara/flickr-image-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8.16G/8.16G [04:46<00:00, 30.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.04G/1.04G [00:36<00:00, 30.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textwrap import wrap\n",
        "\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"dark\")\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-08-23T11:23:20.240075Z",
          "iopub.execute_input": "2022-08-23T11:23:20.240687Z",
          "iopub.status.idle": "2022-08-23T11:23:27.839141Z",
          "shell.execute_reply.started": "2022-08-23T11:23:20.240606Z",
          "shell.execute_reply": "2022-08-23T11:23:27.837973Z"
        },
        "trusted": true,
        "id": "HV-kLab-3GoB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Captioning**\n",
        "\n",
        "**What is Image Captioning ?**\n",
        "- Image Captioning is the process of generating textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions.\n",
        "- This task lies at the intersection of computer vision and natural language processing. Most image captioning systems use an encoder-decoder framework, where an input image is encoded into an intermediate representation of the information in the image, and then decoded into a descriptive text sequence.\n",
        "\n",
        "**CNNs + RNNs (LSTMs)**\n",
        "- To perform Image Captioning we will require two deep learning models combined into one for the training purpose\n",
        "- CNNs extract the features from the image of some vector size aka the vector embeddings. The size of these embeddings depend on the type of pretrained network being used for the feature extraction\n",
        "- LSTMs are used for the text generation process. The image embeddings are concatenated with the word embeddings and passed to the LSTM to generate the next word\n",
        "- For a more illustrative explanation of this architecture check the Modelling section for a picture representation"
      ],
      "metadata": {
        "id": "e7hlRRCZ3GoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/1*6BFOIdSHlk24Z3DFEakvnQ.png\">"
      ],
      "metadata": {
        "id": "R06ho1473GoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '../input/flickr8k/Images'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:23:27.841813Z",
          "iopub.execute_input": "2022-08-23T11:23:27.842707Z",
          "iopub.status.idle": "2022-08-23T11:23:27.846691Z",
          "shell.execute_reply.started": "2022-08-23T11:23:27.842668Z",
          "shell.execute_reply": "2022-08-23T11:23:27.845918Z"
        },
        "trusted": true,
        "id": "qcw0Mkqo3GoE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"../input/flickr8k/captions.txt\")\n",
        "data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:23:27.848057Z",
          "iopub.execute_input": "2022-08-23T11:23:27.848663Z",
          "iopub.status.idle": "2022-08-23T11:23:27.966722Z",
          "shell.execute_reply.started": "2022-08-23T11:23:27.848617Z",
          "shell.execute_reply": "2022-08-23T11:23:27.965974Z"
        },
        "trusted": true,
        "id": "ajC_ZvJJ3GoE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "8aa5eb62-0659-4f93-9cfc-6664157c380f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/flickr8k/captions.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-76b7bc250d31>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/flickr8k/captions.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/flickr8k/captions.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readImage(path,img_size=224):\n",
        "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "\n",
        "    return img\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(15):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
        "        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n",
        "        plt.imshow(image)\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:23:27.968619Z",
          "iopub.execute_input": "2022-08-23T11:23:27.969054Z",
          "iopub.status.idle": "2022-08-23T11:23:27.976892Z",
          "shell.execute_reply.started": "2022-08-23T11:23:27.969018Z",
          "shell.execute_reply": "2022-08-23T11:23:27.975608Z"
        },
        "trusted": true,
        "id": "sPzAZtYj3GoF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization**\n",
        "- Images and their corresponding captions"
      ],
      "metadata": {
        "id": "Gq1VGxBk3GoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(data.sample(15))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:23:27.978551Z",
          "iopub.execute_input": "2022-08-23T11:23:27.978905Z",
          "iopub.status.idle": "2022-08-23T11:23:29.509999Z",
          "shell.execute_reply.started": "2022-08-23T11:23:27.978871Z",
          "shell.execute_reply": "2022-08-23T11:23:29.505799Z"
        },
        "trusted": true,
        "id": "pE6m08hK3GoF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "c9029517-5609-4cb4-8b49-ea975ccd2e16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f8f74f19b337>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Caption Text Preprocessing Steps**\n",
        "- Convert sentences into lowercase\n",
        "- Remove special characters and numbers present in the text\n",
        "- Remove extra spaces\n",
        "- Remove single characters\n",
        "- Add a starting and an ending tag to the sentences to indicate the beginning and the ending of a sentence\n",
        "\n",
        "<img src='http://zjpnrm2br14wspo448nls17u-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/processing-steps.png' >"
      ],
      "metadata": {
        "id": "kncoTnwY3GoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(data):\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
        "    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n",
        "    return data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:23:33.882355Z",
          "iopub.execute_input": "2022-08-23T11:23:33.883049Z",
          "iopub.status.idle": "2022-08-23T11:23:33.889484Z",
          "shell.execute_reply.started": "2022-08-23T11:23:33.883014Z",
          "shell.execute_reply": "2022-08-23T11:23:33.888501Z"
        },
        "trusted": true,
        "id": "yrB1pi663GoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Preprocessed Text__"
      ],
      "metadata": {
        "id": "pE3oV9AX3GoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = text_preprocessing(data)\n",
        "captions = data['caption'].tolist()\n",
        "captions[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:25:32.612283Z",
          "iopub.execute_input": "2022-08-23T11:25:32.612695Z",
          "iopub.status.idle": "2022-08-23T11:25:32.775701Z",
          "shell.execute_reply.started": "2022-08-23T11:25:32.612663Z",
          "shell.execute_reply": "2022-08-23T11:25:32.774736Z"
        },
        "trusted": true,
        "id": "5HsSZXK03GoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Tokenization and Encoded Representation__\n",
        "- The words in a sentence are separated/tokenized and encoded in a one hot representation\n",
        "- These encodings are then passed to the embeddings layer to generate word embeddings\n",
        "\n",
        "<img src='https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif'>"
      ],
      "metadata": {
        "id": "T15UMmkj3GoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in captions)\n",
        "\n",
        "images = data['image'].unique().tolist()\n",
        "nimages = len(images)\n",
        "\n",
        "split_index = round(0.85*nimages)\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "train = data[data['image'].isin(train_images)]\n",
        "test = data[data['image'].isin(val_images)]\n",
        "\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "test.reset_index(inplace=True,drop=True)\n",
        "\n",
        "tokenizer.texts_to_sequences([captions[1]])[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:25:33.019345Z",
          "iopub.execute_input": "2022-08-23T11:25:33.020096Z",
          "iopub.status.idle": "2022-08-23T11:25:33.712285Z",
          "shell.execute_reply.started": "2022-08-23T11:25:33.020062Z",
          "shell.execute_reply": "2022-08-23T11:25:33.711513Z"
        },
        "trusted": true,
        "id": "Fq8EBVLm3GoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Feature Extraction**\n",
        "- DenseNet 201 Architecture is used to extract the features from the images\n",
        "- Any other pretrained architecture can also be used for extracting features from these images\n",
        "- Since the Global Average Pooling layer is selected as the final layer of the DenseNet201 model for our feature extraction, our image embeddings will be a vector of size 1920\n",
        "\n",
        "<img src=\"https://imgur.com/wWHWbQt.jpg\">"
      ],
      "metadata": {
        "id": "DKhut1jo3GoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DenseNet201()\n",
        "fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "img_size = 224\n",
        "features = {}\n",
        "for image in tqdm(data['image'].unique().tolist()):\n",
        "    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "    img = np.expand_dims(img,axis=0)\n",
        "    feature = fe.predict(img, verbose=0)\n",
        "    features[image] = feature"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:25:42.774961Z",
          "iopub.execute_input": "2022-08-23T11:25:42.77532Z",
          "iopub.status.idle": "2022-08-23T11:36:27.403085Z",
          "shell.execute_reply.started": "2022-08-23T11:25:42.77529Z",
          "shell.execute_reply": "2022-08-23T11:36:27.402258Z"
        },
        "trusted": true,
        "id": "WU_UFqrA3GoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Generation**\n",
        "- Since Image Caption model training like any other neural network training is a highly resource utillizing process we cannot load the data into the main memory all at once, and hence we need to generate the data in the required format batch wise\n",
        "- The inputs will be the image embeddings and their corresonding caption text embeddings for the training process\n",
        "- The text embeddings are passed word by word for the caption generation during inference time"
      ],
      "metadata": {
        "id": "4_CcA6Rp3GoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
        "                 vocab_size, max_length, features,shuffle=True):\n",
        "\n",
        "        self.df = df.copy()\n",
        "        self.X_col = X_col\n",
        "        self.y_col = y_col\n",
        "        self.directory = directory\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "        self.features = features\n",
        "        self.shuffle = shuffle\n",
        "        self.n = len(self.df)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n",
        "        X1, X2, y = self.__get_data(batch)\n",
        "        return (X1, X2), y\n",
        "\n",
        "    def __get_data(self,batch):\n",
        "\n",
        "        X1, X2, y = list(), list(), list()\n",
        "\n",
        "        images = batch[self.X_col].tolist()\n",
        "\n",
        "        for image in images:\n",
        "            feature = self.features[image][0]\n",
        "\n",
        "            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n",
        "            for caption in captions:\n",
        "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "                for i in range(1,len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
        "                    X1.append(feature)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "        return X1, X2, y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:08.818593Z",
          "iopub.execute_input": "2022-08-23T11:37:08.818967Z",
          "iopub.status.idle": "2022-08-23T11:37:08.831291Z",
          "shell.execute_reply.started": "2022-08-23T11:37:08.818929Z",
          "shell.execute_reply": "2022-08-23T11:37:08.830339Z"
        },
        "trusted": true,
        "id": "YCPpW6Pf3GoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelling**\n",
        "- The image embedding representations are concatenated with the first word of sentence ie. starseq and passed to the LSTM network\n",
        "- The LSTM network starts generating words after each input thus forming a sentence at the end"
      ],
      "metadata": {
        "id": "Va1GlDHO3GoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png'>"
      ],
      "metadata": {
        "id": "t_WszTCj3GoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = Input(shape=(1920,))\n",
        "input2 = Input(shape=(max_length,))\n",
        "\n",
        "img_features = Dense(256, activation='relu')(input1)\n",
        "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
        "\n",
        "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
        "merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
        "sentence_features = LSTM(256)(merged)\n",
        "x = Dropout(0.5)(sentence_features)\n",
        "x = add([x, img_features])\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "caption_model = Model(inputs=[input1,input2], outputs=output)\n",
        "caption_model.compile(loss='categorical_crossentropy',optimizer='adam')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:09.708506Z",
          "iopub.execute_input": "2022-08-23T11:37:09.708941Z",
          "iopub.status.idle": "2022-08-23T11:37:10.019559Z",
          "shell.execute_reply.started": "2022-08-23T11:37:09.708906Z",
          "shell.execute_reply": "2022-08-23T11:37:10.018731Z"
        },
        "trusted": true,
        "id": "TUFPlIYi3GoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:10.268454Z",
          "iopub.execute_input": "2022-08-23T11:37:10.268931Z",
          "iopub.status.idle": "2022-08-23T11:37:10.276297Z",
          "shell.execute_reply.started": "2022-08-23T11:37:10.268887Z",
          "shell.execute_reply": "2022-08-23T11:37:10.275242Z"
        },
        "trusted": true,
        "id": "JvMqpvWD3GoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Modification**\n",
        "- A slight change has been made in the original model architecture to push the performance. The image feature embeddings are added to the output of the LSTMs and then passed on to the fully connected layers\n",
        "- This slightly improves the performance of the model orignally proposed back in 2014: __Show and Tell: A Neural Image Caption Generator__ (https://arxiv.org/pdf/1411.4555.pdf)"
      ],
      "metadata": {
        "id": "-PIAiImU3GoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(caption_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:10.65206Z",
          "iopub.execute_input": "2022-08-23T11:37:10.652445Z",
          "iopub.status.idle": "2022-08-23T11:37:11.776979Z",
          "shell.execute_reply.started": "2022-08-23T11:37:10.652407Z",
          "shell.execute_reply": "2022-08-23T11:37:11.775955Z"
        },
        "trusted": true,
        "id": "wcSM5IcS3GoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:11.779356Z",
          "iopub.execute_input": "2022-08-23T11:37:11.779788Z",
          "iopub.status.idle": "2022-08-23T11:37:11.787882Z",
          "shell.execute_reply.started": "2022-08-23T11:37:11.779746Z",
          "shell.execute_reply": "2022-08-23T11:37:11.787103Z"
        },
        "trusted": true,
        "id": "GuToSXTT3GoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
        "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n",
        "\n",
        "validation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
        "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:11.854997Z",
          "iopub.execute_input": "2022-08-23T11:37:11.855484Z",
          "iopub.status.idle": "2022-08-23T11:37:11.865903Z",
          "shell.execute_reply.started": "2022-08-23T11:37:11.85545Z",
          "shell.execute_reply": "2022-08-23T11:37:11.864905Z"
        },
        "trusted": true,
        "id": "lmDuRmwU3GoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"model.h5\"\n",
        "checkpoint = ModelCheckpoint(model_name,\n",
        "                            monitor=\"val_loss\",\n",
        "                            mode=\"min\",\n",
        "                            save_best_only = True,\n",
        "                            verbose=1)\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                            patience=3,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.2,\n",
        "                                            min_lr=0.00000001)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:12.583779Z",
          "iopub.execute_input": "2022-08-23T11:37:12.584292Z",
          "iopub.status.idle": "2022-08-23T11:37:12.590643Z",
          "shell.execute_reply.started": "2022-08-23T11:37:12.584258Z",
          "shell.execute_reply": "2022-08-23T11:37:12.589422Z"
        },
        "trusted": true,
        "id": "xL-11g7i3GoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's train the Model !**\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*xIXqf46yYonSXkUOWcOCvg.gif'>"
      ],
      "metadata": {
        "id": "ZypzQV1u3GoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = caption_model.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:37:12.817579Z",
          "iopub.execute_input": "2022-08-23T11:37:12.817939Z",
          "iopub.status.idle": "2022-08-23T11:52:23.539818Z",
          "shell.execute_reply.started": "2022-08-23T11:37:12.817909Z",
          "shell.execute_reply": "2022-08-23T11:52:23.538896Z"
        },
        "trusted": true,
        "id": "NIJy_sSk3GoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**\n",
        "- Learning Curve (Loss Curve)\n",
        "- Assessment of Generated Captions (by checking the relevance of the caption with respect to the image, BLEU Score will not be used in this kernel)"
      ],
      "metadata": {
        "id": "SgLtgV063GoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Learning Curve**\n",
        "- The model has clearly overfit, possibly due to less amount of data\n",
        "- We can tackle this problem in two ways\n",
        "    1. Train the model on a larger dataset Flickr40k\n",
        "    2. Attention Models"
      ],
      "metadata": {
        "id": "g5U2mCFW3GoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:52:36.643783Z",
          "iopub.execute_input": "2022-08-23T11:52:36.644163Z",
          "iopub.status.idle": "2022-08-23T11:52:54.616968Z",
          "shell.execute_reply.started": "2022-08-23T11:52:36.644131Z",
          "shell.execute_reply": "2022-08-23T11:52:54.616186Z"
        },
        "trusted": true,
        "id": "TQlFF07Q3GoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Caption Generation Utility Functions**\n",
        "- Utility functions to generate the captions of input images at the inference time.\n",
        "- Here the image embeddings are passed along with the first word, followed by which the text embedding of each new word is passed to generate the next word"
      ],
      "metadata": {
        "id": "HA7vzrkb3GoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_word(integer,tokenizer):\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index==integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:52:54.618668Z",
          "iopub.execute_input": "2022-08-23T11:52:54.619208Z",
          "iopub.status.idle": "2022-08-23T11:52:54.624359Z",
          "shell.execute_reply.started": "2022-08-23T11:52:54.619169Z",
          "shell.execute_reply": "2022-08-23T11:52:54.623387Z"
        },
        "trusted": true,
        "id": "5GXR-V653GoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption(model, image, tokenizer, max_length, features):\n",
        "\n",
        "    feature = features[image]\n",
        "    in_text = \"startseq\"\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "\n",
        "        y_pred = model.predict([feature,sequence])\n",
        "        y_pred = np.argmax(y_pred)\n",
        "\n",
        "        word = idx_to_word(y_pred, tokenizer)\n",
        "\n",
        "        if word is None:\n",
        "            break\n",
        "\n",
        "        in_text+= \" \" + word\n",
        "\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:52:54.625803Z",
          "iopub.execute_input": "2022-08-23T11:52:54.626208Z",
          "iopub.status.idle": "2022-08-23T11:52:54.635105Z",
          "shell.execute_reply.started": "2022-08-23T11:52:54.626153Z",
          "shell.execute_reply": "2022-08-23T11:52:54.634224Z"
        },
        "trusted": true,
        "id": "F-26vSyi3GoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Taking 15 Random Samples for Caption Prediction**"
      ],
      "metadata": {
        "id": "6NsF0Hap3GoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = test.sample(15)\n",
        "samples.reset_index(drop=True,inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:53:17.519194Z",
          "iopub.execute_input": "2022-08-23T11:53:17.519766Z",
          "iopub.status.idle": "2022-08-23T11:53:17.528148Z",
          "shell.execute_reply.started": "2022-08-23T11:53:17.519727Z",
          "shell.execute_reply": "2022-08-23T11:53:17.526393Z"
        },
        "trusted": true,
        "id": "oOP9wAda3GoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,record in samples.iterrows():\n",
        "\n",
        "    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "\n",
        "    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n",
        "    samples.loc[index,'caption'] = caption"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:53:47.559188Z",
          "iopub.execute_input": "2022-08-23T11:53:47.559724Z",
          "iopub.status.idle": "2022-08-23T11:53:54.421752Z",
          "shell.execute_reply.started": "2022-08-23T11:53:47.559692Z",
          "shell.execute_reply": "2022-08-23T11:53:54.420902Z"
        },
        "trusted": true,
        "id": "0mZ4H2g03GoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "- As we can clearly see there is some redundant caption generation e.g. Dog running through the water, overusage of blue shirt for any other coloured cloth\n",
        "- The model performance can be further improved by training on more data and using attention mechanism so that our model can focus on relevant areas during the text generation\n",
        "- We can also leverage the interprettability of the attention mechanism to understand which areas of the image leads to the generation of which word"
      ],
      "metadata": {
        "id": "QyP7cdQ03GoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(samples)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-23T11:53:56.391394Z",
          "iopub.execute_input": "2022-08-23T11:53:56.391764Z",
          "iopub.status.idle": "2022-08-23T11:53:57.852139Z",
          "shell.execute_reply.started": "2022-08-23T11:53:56.391734Z",
          "shell.execute_reply": "2022-08-23T11:53:57.851198Z"
        },
        "trusted": true,
        "id": "WblDt8bt3GoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style='font-size: 18px'><strong>Conclusion: </strong>This may not be the best performing model, but the objective of this kernel is to give a gist of how Image Captioning problems can be approached. In the future work of this kernel <strong>Attention model</strong> training and <strong>BLEU Score</strong> assessment will be performed.</p>"
      ],
      "metadata": {
        "id": "hpGoAS8g3GoP"
      }
    }
  ]
}